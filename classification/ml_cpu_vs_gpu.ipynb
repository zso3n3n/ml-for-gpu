{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77c3d545",
   "metadata": {},
   "source": [
    "> Reminder: Use the ```rapids-sk``` kernel to run this notebook  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Accelerate ML with GPUs\n",
    "\n",
    "This notebook demonstrates 20-50√ó speedups by migrating CPU workflows (pandas, scikit-learn) to GPU (cuDF, cuML) on Avazu CTR dataset with minimal code changes.\n",
    "\n",
    "**Objectives:**\n",
    "- Compare CPU vs GPU performance on ETL and ML tasks\n",
    "- Measure speedups for read, ETL, fit, predict stages\n",
    "- Verify model parity (AUC/logloss within ¬±0.5%)\n",
    "- Demonstrate minimal migration effort (‚â§5 lines changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cudf\n",
    "_ = cudf.Series([1,2,3]).sum() # warmup\n",
    "from cudf.api.types import is_string_dtype\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.metrics import roc_auc_score as cuml_roc_auc_score\n",
    "from cuml.model_selection import train_test_split as cuml_train_test_split\n",
    "\n",
    "import cupy as cp\n",
    "import rmm.mr as mr\n",
    "from rmm.allocators.cupy import rmm_cupy_allocator\n",
    "from cuml import set_global_output_type\n",
    "\n",
    "from utils.timing import set_cpu_threads, run_timed\n",
    "\n",
    "# Set reproducible seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Configure CPU threads for fair comparison\n",
    "set_cpu_threads(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA async allocator (fast, no manual pool sizing)\n",
    "mr.set_current_device_resource(mr.CudaAsyncMemoryResource())\n",
    "cp.cuda.set_allocator(rmm_cupy_allocator)\n",
    "set_global_output_type(\"cudf\")\n",
    "\n",
    "# Disable RAPIDS initialization\n",
    "os.environ.setdefault(\"RAPIDS_NO_INITIALIZE\", \"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAMPLE = True\n",
    "extract_dir = os.path.join(os.getcwd(), \"data\", \"avazu\")  # Current directory is classification/\n",
    "parquet_path = os.path.join(os.getcwd(), \"data\", \"avazu\", \"avazu_train.parquet\")\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    FILE = os.path.join(os.getcwd(), \"data\", \"avazu-ctr-50k.zip\")  # File is in classification/data/\n",
    "    with zipfile.ZipFile(FILE, 'r') as zip_ref:\n",
    "        csv_file = [f for f in zip_ref.namelist() if f.endswith('.csv')][0]\n",
    "        zip_ref.extract(csv_file, extract_dir)\n",
    "        extracted_csv = os.path.join(extract_dir, csv_file)\n",
    "    print(f\"Extracted SAMPLE data - {os.path.getsize(extracted_csv) / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    FILE = os.path.join(os.getcwd(), \"data\", \"avazu-ctr.gz\")  # File is in classification/data/\n",
    "    extracted_csv = os.path.join(extract_dir, \"avazu-ctr.csv\")\n",
    "    with gzip.open(FILE, 'rb') as f_in, open(extracted_csv, 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f_in.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "    print(f\"Extracted FULL data - {os.path.getsize(extracted_csv) / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "def csv_to_parquet(src_csv, dst_parquet, chunksize=500_000):\n",
    "    writer = None\n",
    "    for chunk in pd.read_csv(src_csv, chunksize=chunksize):\n",
    "        table = pa.Table.from_pandas(chunk, preserve_index=False)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(dst_parquet, table.schema)\n",
    "        writer.write_table(table)\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "csv_to_parquet(extracted_csv, parquet_path)\n",
    "print(f\"Converted CSV to Parquet - {parquet_path}\")\n",
    "print(f\"Shape: {pd.read_parquet(parquet_path).shape}\")\n",
    "os.remove(extracted_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## CPU Pipeline - pandas + scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_pipeline():\n",
    "    \"\"\"ETL pipeline using CPU (pandas) with timing.\"\"\"\n",
    "    # 1. Load data\n",
    "    df_cpu, load_time = run_timed(\n",
    "        \"CPU: Load Parquet with pandas\",\n",
    "        lambda: pd.read_parquet(parquet_path),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 2. Basic preprocessing\n",
    "    df_processed, preprocess_time = run_timed(\n",
    "        \"CPU: Preprocess data (handle missing, encode categories)\",\n",
    "        lambda: preprocess_avazu_data(df_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 3. Feature engineering\n",
    "    (X_cpu, y_cpu), features_time = run_timed(\n",
    "        \"CPU: Feature engineering\",\n",
    "        lambda: engineer_features(df_processed),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 4. Train/test split\n",
    "    (X_train_cpu, X_test_cpu, y_train_cpu, y_test_cpu), split_time = run_timed(\n",
    "        \"CPU: Train/test split\",\n",
    "        lambda: train_test_split(X_cpu, y_cpu, test_size=0.2, random_state=42),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 5. Fit Random Forest Classifier\n",
    "    rf_model, rf_time = run_timed(\n",
    "        \"CPU: Fit Random Forest\",\n",
    "        lambda: RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=16,            # align with GPU\n",
    "            max_features=\"sqrt\",     # align with GPU\n",
    "            bootstrap=True,          # align with GPU\n",
    "            criterion=\"gini\",        # explicit; matches cuML default\n",
    "            random_state=42,\n",
    "            n_jobs=-1                # CPU parallelism; no effect on model definition\n",
    "        ).fit(X_train_cpu, y_train_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 6. Predict probabilities\n",
    "    y_pred_cpu, pred_time = run_timed(\n",
    "        \"CPU: Predict probabilities\",\n",
    "        lambda: rf_model.predict_proba(X_test_cpu)[:, 1],\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 7. Calculate AUC\n",
    "    auc_cpu, auc_time = run_timed(\n",
    "        \"CPU: Calculate AUC\",\n",
    "        lambda: roc_auc_score(y_test_cpu, y_pred_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_cpu,\n",
    "        'X_test': X_test_cpu,\n",
    "        'y_train': y_train_cpu,\n",
    "        'y_test': y_test_cpu,\n",
    "        'model': rf_model,\n",
    "        'predictions': y_pred_cpu,\n",
    "        'auc': auc_cpu,\n",
    "        'times': {\n",
    "            'load': load_time,\n",
    "            'preprocess': preprocess_time,\n",
    "            'features': features_time,\n",
    "            'split': split_time,\n",
    "            'fit': rf_time,\n",
    "            'predict': pred_time,\n",
    "            'auc': auc_time,\n",
    "            'total': load_time + preprocess_time + features_time + split_time + rf_time + pred_time + auc_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Helper functions for preprocessing\n",
    "def preprocess_avazu_data(df):\n",
    "    \"\"\"Basic preprocessing for Avazu dataset.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna('missing')\n",
    "    categorical_cols = [col for col in df_clean.columns if col not in ['click', 'id']]\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = pd.Categorical(df_clean[col]).codes\n",
    "    return df_clean\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Basic feature engineering.\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col not in ['click', 'id']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['click'] if 'click' in df.columns else df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "print(\"üñ•Ô∏è  Running CPU Pipeline...\")\n",
    "cpu_results = cpu_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## GPU Pipeline - cuDF + cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_pipeline():\n",
    "    \"\"\"ML pipeline using GPU (cuDF/cuML) with timing.\"\"\"\n",
    "    # 1) Load data ‚Äî skip 'id' at read\n",
    "    df_gpu, load_time = run_timed(\n",
    "        \"GPU: Load Parquet with cuDF (selected columns)\",\n",
    "        lambda: read_parquet_gpu_selected(parquet_path),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 2) Preprocess\n",
    "    df_processed, preprocess_time = run_timed(\n",
    "        \"GPU: Preprocess data (missing, encode categories)\",\n",
    "        lambda: preprocess_avazu_data_gpu(df_gpu),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 3) Features\n",
    "    (X_gpu, y_gpu), features_time = run_timed(\n",
    "        \"GPU: Feature engineering\",\n",
    "        lambda: engineer_features_gpu(df_processed),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 4) Split on device\n",
    "    (X_train_gpu, X_test_gpu, y_train_gpu, y_test_gpu), split_time = run_timed(\n",
    "        \"GPU: Train/test split\",\n",
    "        lambda: cuml_train_test_split(X_gpu, y_gpu, test_size=0.2, random_state=42),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 5) Fit RF ‚Äî params tuned for T4 throughput; adjust for parity vs speed\n",
    "    rf_params = dict(\n",
    "        n_estimators=100,        # raise for accuracy, lower for speed\n",
    "        max_depth=16,            # T4 sweet spot; 12 is faster, may reduce AUC\n",
    "        max_features=\"sqrt\",     # parity with sklearn default behavior\n",
    "        bootstrap=True,          # set False for extra speed (changes model)\n",
    "        n_bins=64,               # <=128 speeds split finding\n",
    "        n_streams=8,             # T4: 4‚Äì8; default is good, 8 saturates\n",
    "        random_state=42,\n",
    "    )\n",
    "    rf_model, rf_time = run_timed(\n",
    "        \"GPU: Fit Random Forest\",\n",
    "        lambda: cuRF(**rf_params).fit(X_train_gpu, y_train_gpu),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 6) Predict probabilities ‚Äî stay on device\n",
    "    def predict_gpu():\n",
    "        proba = rf_model.predict_proba(X_test_gpu)\n",
    "        # cuML returns cudf.DataFrame for cudf input\n",
    "        return proba.iloc[:, 1] if hasattr(proba, \"iloc\") else proba[:, 1]\n",
    "\n",
    "    y_pred_gpu, pred_time = run_timed(\n",
    "        \"GPU: Predict probabilities\",\n",
    "        predict_gpu,\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 7) AUC fully on GPU (no to_pandas)\n",
    "    auc_gpu, auc_time = run_timed(\n",
    "        \"GPU: Calculate AUC\",\n",
    "        lambda: cuml_roc_auc_score(y_test_gpu, y_pred_gpu),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"X_train\": X_train_gpu,\n",
    "        \"X_test\": X_test_gpu,\n",
    "        \"y_train\": y_train_gpu,\n",
    "        \"y_test\": y_test_gpu,\n",
    "        \"model\": rf_model,\n",
    "        \"predictions\": y_pred_gpu,\n",
    "        \"auc\": auc_gpu,\n",
    "        \"times\": {\n",
    "            \"load\": load_time,\n",
    "            \"preprocess\": preprocess_time,\n",
    "            \"features\": features_time,\n",
    "            \"split\": split_time,\n",
    "            \"fit\": rf_time,\n",
    "            \"predict\": pred_time,\n",
    "            \"auc\": auc_time,\n",
    "            \"total\": load_time + preprocess_time + features_time + split_time + rf_time + pred_time + auc_time,\n",
    "        },\n",
    "    }\n",
    "\n",
    "# Helper functions for GPU preprocessing\n",
    "def read_parquet_gpu_selected(path):\n",
    "    # Avoid reading unused columns to cut IO and device memory\n",
    "    schema = pq.read_schema(path)\n",
    "    cols = [name for name in schema.names if name != \"id\"]\n",
    "    return cudf.read_parquet(path, columns=cols)\n",
    "\n",
    "def preprocess_avazu_data_gpu(df):\n",
    "    \"\"\"Basic preprocessing for Avazu dataset using cuDF.\"\"\"\n",
    "    df = df.copy(deep=False)\n",
    "    df = df.fillna(\"missing\")\n",
    "    # Encode string columns to int32 codes on device\n",
    "    str_cols = [c for c in df.columns if c != \"click\" and is_string_dtype(df[c])]\n",
    "    for c in str_cols:\n",
    "        df[c] = cudf.factorize(df[c])[0].astype(\"int32\")\n",
    "    return df\n",
    "\n",
    "def engineer_features_gpu(df):\n",
    "    \"\"\"Basic feature engineering using cuDF.\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col not in [\"click\", \"id\"]]\n",
    "    X = df[feature_cols]  # keep int32; cuML handles it\n",
    "    y = df[\"click\"] if \"click\" in df.columns else df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "print(\"üöÄ Running GPU Pipeline...\")\n",
    "gpu_results = gpu_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Performance Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: AUC (higher is better), times (lower is better)\n",
    "metrics = [\n",
    "    \"AUC\",\n",
    "    \"Preprocess Time (s)\",\n",
    "    \"Feature Time (s)\",\n",
    "    \"Fit Time (s)\",\n",
    "    \"Predict Time (s)\",\n",
    "    \"AUC Time (s)\",\n",
    "    \"Total Time (s)\",\n",
    "]\n",
    "cpu_vals = [\n",
    "    cpu_results[\"auc\"],\n",
    "    cpu_results[\"times\"][\"preprocess\"],\n",
    "    cpu_results[\"times\"][\"features\"],\n",
    "    cpu_results[\"times\"][\"fit\"],\n",
    "    cpu_results[\"times\"][\"predict\"],\n",
    "    cpu_results[\"times\"][\"auc\"],\n",
    "    cpu_results[\"times\"][\"total\"],\n",
    "]\n",
    "gpu_vals = [\n",
    "    gpu_results[\"auc\"],\n",
    "    gpu_results[\"times\"][\"preprocess\"],\n",
    "    gpu_results[\"times\"][\"features\"],\n",
    "    gpu_results[\"times\"][\"fit\"],\n",
    "    gpu_results[\"times\"][\"predict\"],\n",
    "    gpu_results[\"times\"][\"auc\"],\n",
    "    gpu_results[\"times\"][\"total\"],\n",
    "]\n",
    "\n",
    "import numpy as np\n",
    "rows = []\n",
    "for metric, cpu, gpu in zip(metrics, cpu_vals, gpu_vals):\n",
    "    if metric == \"AUC\":\n",
    "        # Accuracy metric: higher is better\n",
    "        diff = (gpu - cpu) if cpu is not None else np.nan\n",
    "        pct_change = ((gpu - cpu) / abs(cpu) * 100) if isinstance(cpu, (int, float, np.floating)) and cpu not in (0, None) else np.nan\n",
    "        speedup = np.nan  # not meaningful for accuracy\n",
    "    else:\n",
    "        # Time metrics: lower is better\n",
    "        diff = (cpu - gpu) if None not in (cpu, gpu) else np.nan\n",
    "        speedup = (cpu / gpu) if all(isinstance(x, (int, float, np.floating)) for x in (cpu, gpu)) and gpu not in (0, None) else np.nan\n",
    "        pct_change = ((1 - (gpu / cpu)) * 100) if isinstance(cpu, (int, float, np.floating)) and cpu not in (0, None) else np.nan\n",
    "    rows.append({\n",
    "        \"Metric\": metric,\n",
    "        \"CPU\": cpu,\n",
    "        \"GPU\": gpu,\n",
    "        \"Diff\": diff,\n",
    "        \"Speedup (CPU/GPU)\": speedup,\n",
    "        \"% Change/Improvement\": pct_change\n",
    "    })\n",
    "\n",
    "results_table = pd.DataFrame(rows)\n",
    "\n",
    "# Formatting: round to two decimals; add % sign to percent column\n",
    "def fmt_float(v):\n",
    "    try:\n",
    "        if v is None or (isinstance(v, float) and (np.isnan(v) or np.isinf(v))):\n",
    "            return \"\"\n",
    "        return f\"{v:.2f}\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def fmt_pct(v):\n",
    "    try:\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)):\n",
    "            return \"\"\n",
    "        return f\"{v:.2f}%\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "display_table = results_table.copy()\n",
    "for col in [\"CPU\", \"GPU\", \"Diff\", \"Speedup (CPU/GPU)\"]:\n",
    "    display_table[col] = display_table[col].apply(fmt_float)\n",
    "display_table[\"% Change/Improvement\"] = display_table[\"% Change/Improvement\"].apply(fmt_pct)\n",
    "\n",
    "print(display_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-sk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
