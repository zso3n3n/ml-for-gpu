{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Accelerate ML with GPUs\n",
    "\n",
    "This notebook demonstrates 20-50Ã— speedups by migrating CPU workflows (pandas, scikit-learn) to GPU (cuDF, cuML) on Avazu CTR dataset with minimal code changes.\n",
    "\n",
    "**Objectives:**\n",
    "- Compare CPU vs GPU performance on ETL and ML tasks\n",
    "- Measure speedups for read, ETL, fit, predict stages\n",
    "- Verify model parity (AUC/logloss within Â±0.5%)\n",
    "- Demonstrate minimal migration effort (â‰¤5 lines changed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import gzip\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import cudf\n",
    "from cuml.ensemble import RandomForestClassifier as cuRF\n",
    "from cuml.metrics import roc_auc_score as cuml_roc_auc_score\n",
    "from cuml.model_selection import train_test_split as cuml_train_test_split\n",
    "\n",
    "from utils.timing import set_cpu_threads, run_timed\n",
    "\n",
    "# Set reproducible seed\n",
    "np.random.seed(123)\n",
    "\n",
    "# Configure CPU threads for fair comparison\n",
    "set_cpu_threads(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SAMPLE = False\n",
    "extract_dir = os.path.join(os.getcwd(), \"data\", \"avazu\")  # Current directory is classification/\n",
    "parquet_path = os.path.join(os.getcwd(), \"data\", \"avazu\", \"avazu_train.parquet\")\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    FILE = os.path.join(os.getcwd(), \"data\", \"avazu-ctr-50k.zip\")  # File is in classification/data/\n",
    "    with zipfile.ZipFile(FILE, 'r') as zip_ref:\n",
    "        csv_file = [f for f in zip_ref.namelist() if f.endswith('.csv')][0]\n",
    "        zip_ref.extract(csv_file, extract_dir)\n",
    "        extracted_csv = os.path.join(extract_dir, csv_file)\n",
    "    print(f\"Extracted SAMPLE data - {os.path.getsize(extracted_csv) / (1024 ** 3):.2f} GB\")\n",
    "else:\n",
    "    FILE = os.path.join(os.getcwd(), \"data\", \"avazu-ctr.gz\")  # File is in classification/data/\n",
    "    extracted_csv = os.path.join(extract_dir, \"avazu-ctr.csv\")\n",
    "    with gzip.open(FILE, 'rb') as f_in, open(extracted_csv, 'wb') as f_out:\n",
    "        while True:\n",
    "            chunk = f_in.read(1024 * 1024)\n",
    "            if not chunk:\n",
    "                break\n",
    "            f_out.write(chunk)\n",
    "    print(f\"Extracted FULL data - {os.path.getsize(extracted_csv) / (1024 ** 3):.2f} GB\")\n",
    "\n",
    "def csv_to_parquet(src_csv, dst_parquet, chunksize=500_000):\n",
    "    writer = None\n",
    "    for chunk in pd.read_csv(src_csv, chunksize=chunksize):\n",
    "        table = pa.Table.from_pandas(chunk)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(dst_parquet, table.schema)\n",
    "        writer.write_table(table)\n",
    "    if writer:\n",
    "        writer.close()\n",
    "\n",
    "csv_to_parquet(extracted_csv, parquet_path)\n",
    "print(f\"Converted CSV to Parquet - {parquet_path}\")\n",
    "print(f\"Shape: {pd.read_parquet(parquet_path).shape}\")\n",
    "os.remove(extracted_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015109c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "pf = pq.ParquetFile(parquet_path)\n",
    "# take first row-group as sample\n",
    "tbl = pf.read_row_group(0)\n",
    "sample_pd = tbl.to_pandas()\n",
    "bytes_per_row = sample_pd.memory_usage(deep=True).sum() / len(sample_pd)\n",
    "total_rows = sum(pf.metadata.row_group(i).num_rows for i in range(pf.num_row_groups))\n",
    "est_bytes = bytes_per_row * total_rows\n",
    "print(\"Estimated bytes in memory:\", est_bytes)\n",
    "print(\"Estimated GiB:\", est_bytes / (1024**3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## CPU Pipeline - pandas + scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_pipeline():\n",
    "    \"\"\"ETL pipeline using CPU (pandas) with timing.\"\"\"\n",
    "    # 1. Load data\n",
    "    df_cpu, load_time = run_timed(\n",
    "        \"CPU: Load Parquet with pandas\",\n",
    "        lambda: pd.read_parquet(parquet_path),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 2. Basic preprocessing\n",
    "    df_processed, preprocess_time = run_timed(\n",
    "        \"CPU: Preprocess data (handle missing, encode categories)\",\n",
    "        lambda: preprocess_avazu_data(df_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 3. Feature engineering\n",
    "    (X_cpu, y_cpu), features_time = run_timed(\n",
    "        \"CPU: Feature engineering\",\n",
    "        lambda: engineer_features(df_processed),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 4. Train/test split\n",
    "    (X_train_cpu, X_test_cpu, y_train_cpu, y_test_cpu), split_time = run_timed(\n",
    "        \"CPU: Train/test split\",\n",
    "        lambda: train_test_split(X_cpu, y_cpu, test_size=0.2, random_state=42),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 5. Fit Random Forest Classifier\n",
    "    rf_model, rf_time = run_timed(\n",
    "        \"CPU: Fit Random Forest\",\n",
    "        lambda: RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1).fit(X_train_cpu, y_train_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 6. Predict probabilities\n",
    "    y_pred_cpu, pred_time = run_timed(\n",
    "        \"CPU: Predict probabilities\",\n",
    "        lambda: rf_model.predict_proba(X_test_cpu)[:, 1],\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    # 7. Calculate AUC\n",
    "    auc_cpu, auc_time = run_timed(\n",
    "        \"CPU: Calculate AUC\",\n",
    "        lambda: roc_auc_score(y_test_cpu, y_pred_cpu),\n",
    "        use_gpu=False\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_cpu,\n",
    "        'X_test': X_test_cpu,\n",
    "        'y_train': y_train_cpu,\n",
    "        'y_test': y_test_cpu,\n",
    "        'model': rf_model,\n",
    "        'predictions': y_pred_cpu,\n",
    "        'auc': auc_cpu,\n",
    "        'times': {\n",
    "            'load': load_time,\n",
    "            'preprocess': preprocess_time,\n",
    "            'features': features_time,\n",
    "            'split': split_time,\n",
    "            'fit': rf_time,\n",
    "            'predict': pred_time,\n",
    "            'auc': auc_time,\n",
    "            'total': load_time + preprocess_time + features_time + split_time + rf_time + pred_time + auc_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Helper functions for preprocessing\n",
    "def preprocess_avazu_data(df):\n",
    "    \"\"\"Basic preprocessing for Avazu dataset.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna('missing')\n",
    "    categorical_cols = [col for col in df_clean.columns if col not in ['click', 'id']]\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = pd.Categorical(df_clean[col]).codes\n",
    "    return df_clean\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"Basic feature engineering.\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col not in ['click', 'id']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['click'] if 'click' in df.columns else df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "print(\"ðŸ–¥ï¸  Running CPU Pipeline...\")\n",
    "cpu_results = cpu_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## GPU Pipeline - cuDF + cuML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_pipeline():\n",
    "    \"\"\"ETL pipeline using GPU (cuDF/cuML) with timing.\"\"\"\n",
    "    # 1. Load data\n",
    "    df_gpu, load_time = run_timed(\n",
    "        \"GPU: Load Parquet with cuDF\",\n",
    "        lambda: cudf.read_parquet(parquet_path),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 2. Basic preprocessing\n",
    "    df_processed, preprocess_time = run_timed(\n",
    "        \"GPU: Preprocess data (handle missing, encode categories)\",\n",
    "        lambda: preprocess_avazu_data_gpu(df_gpu),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 3. Feature engineering\n",
    "    (X_gpu, y_gpu), features_time = run_timed(\n",
    "        \"GPU: Feature engineering\",\n",
    "        lambda: engineer_features_gpu(df_processed),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 4. Train/test split\n",
    "    (X_train_gpu, X_test_gpu, y_train_gpu, y_test_gpu), split_time = run_timed(\n",
    "        \"GPU: Train/test split\",\n",
    "        lambda: cuml_train_test_split(X_gpu, y_gpu, test_size=0.2, random_state=42),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 5. Fit Random Forest Classifier\n",
    "    rf_model, rf_time = run_timed(\n",
    "        \"GPU: Fit Random Forest\",\n",
    "        lambda: cuRF(n_estimators=100, random_state=42).fit(X_train_gpu, y_train_gpu),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 6. Predict probabilities - Handle cuML predict_proba differently\n",
    "    def predict_gpu():\n",
    "        proba = rf_model.predict_proba(X_test_gpu)\n",
    "        if hasattr(proba, 'iloc'):\n",
    "            return proba.iloc[:, 1] if proba.shape[1] > 1 else proba.iloc[:, 0]\n",
    "        else:\n",
    "            return proba[:, 1] if proba.shape[1] > 1 else proba[:, 0]\n",
    "    \n",
    "    y_pred_gpu, pred_time = run_timed(\n",
    "        \"GPU: Predict probabilities\",\n",
    "        predict_gpu,\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    # 7. Calculate AUC (convert to pandas for compatibility if needed)\n",
    "    auc_gpu, auc_time = run_timed(\n",
    "        \"GPU: Calculate AUC\",\n",
    "        lambda: cuml_roc_auc_score(\n",
    "            y_test_gpu.to_pandas() if hasattr(y_test_gpu, 'to_pandas') else y_test_gpu,\n",
    "            y_pred_gpu.to_pandas() if hasattr(y_pred_gpu, 'to_pandas') else y_pred_gpu\n",
    "        ),\n",
    "        use_gpu=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'X_train': X_train_gpu,\n",
    "        'X_test': X_test_gpu,\n",
    "        'y_train': y_train_gpu,\n",
    "        'y_test': y_test_gpu,\n",
    "        'model': rf_model,\n",
    "        'predictions': y_pred_gpu,\n",
    "        'auc': auc_gpu,\n",
    "        'times': {\n",
    "            'load': load_time,\n",
    "            'preprocess': preprocess_time,\n",
    "            'features': features_time,\n",
    "            'split': split_time,\n",
    "            'fit': rf_time,\n",
    "            'predict': pred_time,\n",
    "            'auc': auc_time,\n",
    "            'total': load_time + preprocess_time + features_time + split_time + rf_time + pred_time + auc_time\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Helper functions for GPU preprocessing\n",
    "def preprocess_avazu_data_gpu(df):\n",
    "    \"\"\"Basic preprocessing for Avazu dataset using cuDF.\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    df_clean = df_clean.fillna('missing')\n",
    "    categorical_cols = [col for col in df_clean.columns if col not in ['click', 'id']]\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = df_clean[col].astype('category').cat.codes\n",
    "    return df_clean\n",
    "\n",
    "def engineer_features_gpu(df):\n",
    "    \"\"\"Basic feature engineering using cuDF.\"\"\"\n",
    "    feature_cols = [col for col in df.columns if col not in ['click', 'id']]\n",
    "    X = df[feature_cols]\n",
    "    y = df['click'] if 'click' in df.columns else df.iloc[:, 0]\n",
    "    return X, y\n",
    "\n",
    "print(\"ðŸš€ Running GPU Pipeline...\")\n",
    "gpu_results = gpu_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Performance Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"AUC\",\n",
    "    \"Preprocess Time (s)\",\n",
    "    \"Feature Time (s)\",\n",
    "    \"Fit Time (s)\",\n",
    "    \"Predict Time (s)\",\n",
    "    \"AUC Time (s)\",\n",
    "    \"Total Time (s)\"\n",
    "]\n",
    "cpu_vals = [\n",
    "    cpu_results['auc'],\n",
    "    cpu_results['times']['preprocess'],\n",
    "    cpu_results['times']['features'],\n",
    "    cpu_results['times']['fit'],\n",
    "    cpu_results['times']['predict'],\n",
    "    cpu_results['times']['auc'],\n",
    "    cpu_results['times']['total']\n",
    "]\n",
    "gpu_vals = [\n",
    "    gpu_results['auc'],\n",
    "    gpu_results['times']['preprocess'],\n",
    "    gpu_results['times']['features'],\n",
    "    gpu_results['times']['fit'],\n",
    "    gpu_results['times']['predict'],\n",
    "    gpu_results['times']['auc'],\n",
    "    gpu_results['times']['total']\n",
    "]\n",
    "\n",
    "diff = [cpu - gpu for cpu, gpu in zip(cpu_vals, gpu_vals)]\n",
    "percent_improvement = [\n",
    "    round(((cpu - gpu) / cpu * 100), 2) if isinstance(cpu, (int, float)) and cpu != 0 else 0\n",
    "    for cpu, gpu in zip(cpu_vals, gpu_vals)\n",
    "]\n",
    "\n",
    "results_table = pd.DataFrame({\n",
    "    \"Metric\": metrics,\n",
    "    \"CPU\": cpu_vals,\n",
    "    \"GPU\": gpu_vals,\n",
    "    \"Diff (CPU-GPU)\": diff,\n",
    "    \"GPU % Improvement\": percent_improvement\n",
    "})\n",
    "\n",
    "print(\"ðŸ”¬ Performance Comparison Table\")\n",
    "print(results_table.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-sk1",
   "language": "python",
   "name": "rapids-sk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
